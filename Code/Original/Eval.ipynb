{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mohit19014/Hostility Detection/Code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mohit19014/Hostility Detection\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3600,
     "status": "ok",
     "timestamp": 1612536754035,
     "user": {
      "displayName": "Mohit Bhardwaj",
      "photoUrl": "",
      "userId": "14164170441677042335"
     },
     "user_tz": -330
    },
    "id": "niEcDBXeYD1w",
    "outputId": "4b57782c-6822-4e42-820c-b7e2643e46d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34mASL\u001b[0m/    \u001b[01;34mData\u001b[0m/   \u001b[01;34m'Final Experiments'\u001b[0m/   \u001b[01;34mPhotos\u001b[0m/      \u001b[01;34mTensorboard\u001b[0m/\n",
      " \u001b[01;34mCode\u001b[0m/   \u001b[01;34mDumps\u001b[0m/   \u001b[01;34mModels\u001b[0m/               sample.tex\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4206,
     "status": "ok",
     "timestamp": 1612536754770,
     "user": {
      "displayName": "Mohit Bhardwaj",
      "photoUrl": "",
      "userId": "14164170441677042335"
     },
     "user_tz": -330
    },
    "id": "QgRaDaGSYD1w",
    "outputId": "c97a1972-bec2-4c5c-c6e7-9cfd57b367aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 10 08:26:04 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    25W / 250W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGjxEg0o09yv",
    "outputId": "c1af34ce-a052-4f87-9bc4-a5b0b6707394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ASL    Data   'Final Experiments'   Photos\t  Tensorboard\n",
      " Code   Dumps   Models\t\t     sample.tex\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General\n",
    "import re\n",
    "import copy\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "### Sklearn\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "### Transformers\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwCd_QU2YD1y"
   },
   "source": [
    "#### Configuration Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GrvaW49RYD1y"
   },
   "outputs": [],
   "source": [
    "class Config1:\n",
    "    def __init__(self):\n",
    "        super(Config1, self).__init__()\n",
    "\n",
    "        self.SEED       = 42\n",
    "#         self.MODEL_PATH = 'Models/IndicBert'#'ai4bharat/indic-bert'\n",
    "#         self.MODEL_PATH = 'Models/IndicBert'\n",
    "        self.MODEL_PATH = 'monsoon-nlp/hindi-bert'\n",
    "\n",
    "\n",
    "        # data\n",
    "        self.PERCENTAGE_DATA  = 100\n",
    "        self.TOKENIZER        = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n",
    "        self.MAX_LENGTH       = 128\n",
    "        self.BATCH_SIZE       = 16\n",
    "        self.VALIDATION_SPLIT = 0.10\n",
    "        self.PREPROCESS_INPUT = True\n",
    "\n",
    "        # Coarse-grained or Fine-Grained\n",
    "        self.NUM_LABELS           = 3                                        ### 1:CG   2:Combine    4:FG\n",
    "        self.COARSE_GRAINED       = True if self.NUM_LABELS ==1 else False \n",
    "        self.COARSE_GRAINED_CLASS = 'Hostile'                                ### 'Fake' 'Defamation' 'Hate' or 'Offensive' \n",
    "        \n",
    "        # Combining Hostile Dimensions\n",
    "        self.COMBINE_HATE_OFFENSIVE  = False     #True if self.NUM_LABELS ==2 else False\n",
    "        self.COMBINE_DEFAMATION_FAKE = False     #if self.NUM_LABELS ==2 else False\n",
    "\n",
    "\n",
    "        ###Features to include\n",
    "        self.SUPERVISED_LEXICON_ATTENTION = True\n",
    "        self.HOSTILITY_LEXICON            = True\n",
    "        self.LEXICON_AVERAGE              = False\n",
    "        self.EMOJI_INFO                   = True\n",
    "        self.EMBED_EMOJI                  = False    ### Doesnot do preprocessing, just adds emoji text into the post\n",
    "        self.HASHTAG_INFO                 = False\n",
    "\n",
    "        # model\n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.NUMBER_OF_BERT_LAYERS = 1\n",
    "        self.FULL_FINETUNING = True\n",
    "        self.LR = 0.01   #2e-5\n",
    "        self.OPTIMIZER = 'AdamW'\n",
    "        # class_weight = torch.FloatTensor([2.0])\n",
    "        # self.CRITERION = torch.nn.BCEWithLogitsLoss(pos_weight=class_weight)\n",
    "        self.CRITERION = 'CrossEntropyLoss'#'BCEWithLogitsLoss'\n",
    "        self.OUTPUT_HIDDEN_STATE=True\n",
    "        self.OUTPUT_ATTENTIONS = True\n",
    "        self.EPOCHS = 5\n",
    "\n",
    "        \n",
    "        ### Loss Hyperparameters\n",
    "        self.DEFAMATION_LOSS_LAMBDA = 1.0\n",
    "        self.FAKE_LOSS_LAMBDA       = 1.0\n",
    "        self.HATE_LOSS_LAMBDA       = 1.0\n",
    "        self.OFFENSIVE_LOSS_LAMBDA  = 1.0\n",
    "\n",
    "        \n",
    "        ### Load Checkpoint\n",
    "        self.SAVE_BEST_ONLY   = True\n",
    "        self.LOAD_CHECKPOINT  = False\n",
    "        self.FINE_TUNE_COARSE = False\n",
    "        self.CHECKPOINT_PATH  = \"\"\n",
    "        self.MODEL_FOLDER     = \"Models/Supervised Lexicon Attention/\"\n",
    "\n",
    "        ### Visualization\n",
    "        self.VISUALIZE_EMBEDDINGS = False\n",
    "        self.PLOT_LOSS = True\n",
    "\n",
    "        ### Evaluation \n",
    "        self.THRESHOLD = 0.5\n",
    "\n",
    "config = Config1()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# warnings.filterwarnings(\"ignore\", category=ResourceWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUqw7BgJ00JF",
    "outputId": "ab306ee8-4371-4163-e7de-60d4e035ab5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(config.NUM_LABELS)\n",
    "print(config.COARSE_GRAINED)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5554,
     "status": "ok",
     "timestamp": 1612536982827,
     "user": {
      "displayName": "Mohit Bhardwaj",
      "photoUrl": "",
      "userId": "14164170441677042335"
     },
     "user_tz": -330
    },
    "id": "rel05_NYYqwz"
   },
   "outputs": [],
   "source": [
    "def get_texts(df):\n",
    "    if(config.EMBED_EMOJI):\n",
    "        return df['replace_emoji']\n",
    "    else:\n",
    "        return df['Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_texts(df):\n",
    "    if(config.EMBED_EMOJI):\n",
    "        return df['replace_emoji']\n",
    "    else:\n",
    "        return df['Tokenized Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 5384,
     "status": "ok",
     "timestamp": 1612536982828,
     "user": {
      "displayName": "Mohit Bhardwaj",
      "photoUrl": "",
      "userId": "14164170441677042335"
     },
     "user_tz": -330
    },
    "id": "xuJI5MudYqwz"
   },
   "outputs": [],
   "source": [
    "def get_labels(df):\n",
    "    if(config.COARSE_GRAINED):\n",
    "        return [[float(i)] for i in df[config.COARSE_GRAINED_CLASS]]  \n",
    "    \n",
    "    elif(config.COMBINE_HATE_OFFENSIVE):\n",
    "        labels = []\n",
    "        for i in range(len(df)):\n",
    "            label = []\n",
    "            label.append(df['Hate'][i])\n",
    "            label.append(df['Offensive'][i])\n",
    "            \n",
    "            if(df['Hate'][i]==0 and df['Offensive'][i]==0):\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)\n",
    "            label = [float(i) for i in label]\n",
    "            labels.append(label)\n",
    "        return labels\n",
    "    \n",
    "    elif(config.COMBINE_DEFAMATION_FAKE):\n",
    "        labels = []\n",
    "        for i in range(len(df)):\n",
    "            label = []\n",
    "            label.append(df['Defamation'][i])\n",
    "            label.append(df['Fake'][i])\n",
    "            \n",
    "            if(df['Defamation'][i]==0 and df['Fake'][i]==0):\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)\n",
    "            \n",
    "            label = [float(i) for i in label]\n",
    "            labels.append(label)\n",
    "        return labels\n",
    "    \n",
    "    else:\n",
    "        labels = []\n",
    "        for i in range(len(df)):\n",
    "            label = []\n",
    "            label.append(df['Defamation'][i])\n",
    "            label.append(df['Fake'][i])\n",
    "            label.append(df['Hate'][i])\n",
    "            label.append(df['Offensive'][i])\n",
    "            label = [float(i) for i in label]\n",
    "            labels.append(label)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_attention_vectors(df):\n",
    "    gold_attn_vectors = []\n",
    "    for i in range(len(df)):\n",
    "        gold_attn_vector = []\n",
    "        gold_attn_vector.append(df['Defamation Gold Attention'][i])\n",
    "        gold_attn_vector.append(df['Fake Gold Attention'][i])\n",
    "        gold_attn_vector.append(df['Hate Gold Attention'][i])\n",
    "        gold_attn_vector.append(df['Offensive Gold Attention'][i])\n",
    "        gold_attn_vectors.append(gold_attn_vector)\n",
    "    return gold_attn_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoji_vectors(df):\n",
    "    return df['emoji2vec'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4866,
     "status": "ok",
     "timestamp": 1612536982831,
     "user": {
      "displayName": "Mohit Bhardwaj",
      "photoUrl": "",
      "userId": "14164170441677042335"
     },
     "user_tz": -330
    },
    "id": "HP0mr5YihDc1"
   },
   "outputs": [],
   "source": [
    "def get_lexicons(df):\n",
    "    if(config.LEXICON_AVERAGE):\n",
    "        return df['Softmaxed Hostility Lexicon Average'].values ### returns [len(df], 4]   sized array\n",
    "    return df['Softmaxed Hostility Lexicon Padded'].values   ### returns [len(df), 200] sized array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4463,
     "status": "ok",
     "timestamp": 1612536982833,
     "user": {
      "displayName": "Mohit Bhardwaj",
      "photoUrl": "",
      "userId": "14164170441677042335"
     },
     "user_tz": -330
    },
    "id": "pY7eiyHGYqwz"
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        text = df['Post'][i]\n",
    "        text = text.lower()\n",
    "        \n",
    "        ### Converting Every URL to https://someurl\n",
    "        text = re.sub('http[a-zA-Z0-9./:]*', 'https://someurl',text)\n",
    "        \n",
    "        ### Converting Every User Mention to @Someuser\n",
    "        text = re.sub('@[a-zA-Z0-9_]*', '@someuser', text)\n",
    "        \n",
    "        ### Removing Emojis\n",
    "        emoji = re.compile(\"[\"      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                                    u\"\\U00002702-\\U000027B0\"\n",
    "                                    u\"\\U00002702-\\U000027B0\"\n",
    "                                    u\"\\U000024C2-\\U0001F251\"\n",
    "                                    u\"\\U0001f926-\\U0001f937\"\n",
    "                                    u\"\\U00010000-\\U0010ffff\"\n",
    "                                    u\"\\u2640-\\u2642\"\n",
    "                                    u\"\\u2600-\\u2B55\"\n",
    "                                    u\"\\u200d\"\n",
    "                                    u\"\\u23cf\"\n",
    "                                    u\"\\u23e9\"\n",
    "                                    u\"\\u231a\"\n",
    "                                    u\"\\ufe0f\"  # dingbats\n",
    "                                    u\"\\u3030\"\n",
    "                                    \"]+\", flags=re.UNICODE)\n",
    "        text =  emoji.sub(r'', text)\n",
    "        text = ' '.join([word[1:] if word[0] == '#' else word for word in text.split()])\n",
    "        \n",
    "        ### Removing Punctuations\n",
    "        table = str.maketrans(\"\",\"\", string.punctuation)\n",
    "        text = text.translate(table)\n",
    "\n",
    "        df.at[i,'Post'] = text\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4339,
     "status": "ok",
     "timestamp": 1612536982834,
     "user": {
      "displayName": "Mohit Bhardwaj",
      "photoUrl": "",
      "userId": "14164170441677042335"
     },
     "user_tz": -330
    },
    "id": "sPLtDb0DYqwz"
   },
   "outputs": [],
   "source": [
    "def plot_tsne(embeddings,labels):\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "    outer_cname = { '0' : {1: 'Defamation', 0:'Non-Defamation'},\n",
    "                    '1' : {1: 'Fake', 0:'Non-Fake'},\n",
    "                    '2' : {1: 'Hate', 0:'Non-Hate'},\n",
    "                    '3' : {1: 'Offensive', 0:'Non-Offensive'},\n",
    "                    '4' : {1: 'Hostile', 0:'Non-Hostile'}}\n",
    "\n",
    "    class_mapping = {'Defamation': '0',\n",
    "                     'Fake': '0',\n",
    "                     'Hate': '2',\n",
    "                     'Offensive': '3',\n",
    "                     'Hostile': '4'}\n",
    "\n",
    "    if(config.COARSE_GRAINED):\n",
    "        transformed = tsne.fit_transform(embeddings)\n",
    "        fig, ax = plt.subplots()\n",
    "        for label in np.unique(labels): \n",
    "            indices = [i for i, l in enumerate(labels) if l == label]\n",
    "            x = np.take(transformed[:,0], indices)\n",
    "            y = np.take(transformed[:,1], indices)\n",
    "\n",
    "            cdict = {1: 'red', 0: 'blue'}\n",
    "            cname = outer_cname[class_mapping[config.COARSE_GRAINED_CLASS]]\n",
    "            ax.scatter(x,y, color = cdict[label], label=cname[label])\n",
    "        \n",
    "        ax.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        transformed = tsne.fit_transform(embeddings)\n",
    "        fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,12))\n",
    "\n",
    "        \n",
    "        counter = 0\n",
    "        all_labels = labels      \n",
    "        for row in ax:\n",
    "            for col in row:\n",
    "                labels  = all_labels[:,counter]\n",
    "                for label in np.unique(labels): \n",
    "                    indices = [i for i, l in enumerate(labels) if l == label]\n",
    "                    x = np.take(transformed[:,0], indices)\n",
    "                    y = np.take(transformed[:,1], indices)\n",
    "\n",
    "                    cdict = {1: 'red', 0: 'blue'}\n",
    "                    cname = outer_cname[str(counter)]\n",
    "                    col.scatter(x,y, color = cdict[label], label=cname[label], alpha =0.5)\n",
    "                    col.legend(loc='best')\n",
    "                counter += 1\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAPkyh2tYqw0"
   },
   "source": [
    "<h3>Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3617,
     "status": "ok",
     "timestamp": 1612536982835,
     "user": {
      "displayName": "Mohit Bhardwaj",
      "photoUrl": "",
      "userId": "14164170441677042335"
     },
     "user_tz": -330
    },
    "id": "KXQcR-H3Yqw1"
   },
   "outputs": [],
   "source": [
    "class HindiHostilityDataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        super(HindiHostilityDataset, self).__init__()\n",
    "\n",
    "        df          = pickle.load(open(dataset_path, 'rb'))\n",
    "        \n",
    "#         if(config.COARSE_GRAINED == False):\n",
    "#             df = df[df['Hostile']==1]\n",
    "#             df = df.reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        num_samples = int(config.PERCENTAGE_DATA*len(df)/100)\n",
    "        df          = df.iloc[:num_samples,:] \n",
    "        \n",
    "        print(\"Taking \",num_samples,\" only\")\n",
    "        if(config.PREPROCESS_INPUT):\n",
    "            df = preprocess(df)\n",
    "\n",
    "           \n",
    "        self.posts             = get_texts(df)\n",
    "        self.tokenized_posts   = get_tokenized_texts(df)\n",
    "        self.labels            = get_labels(df)\n",
    "        self.gold_attn_vectors = get_gold_attention_vectors(df) ### Value at an index is a list four 50 dim vectors\n",
    "        self.lexicon_vectors   = get_lexicons(df)      ### A vector of 50*4 size or 4 size vector\n",
    "        self.emoji_vectors     = get_emoji_vectors(df) ### A 300 dimension mean vector\n",
    "        self.max_length        = config.MAX_LENGTH\n",
    "        self.tokenizer         = config.TOKENIZER\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        tokenized_conts = self.tokenizer.encode_plus( self.posts[index], \n",
    "                                                      max_length=self.max_length,\n",
    "                                                      padding='max_length',\n",
    "                                                      truncation=True,\n",
    "                                                      return_attention_mask=True,\n",
    "                                                      return_token_type_ids=False,\n",
    "                                                      return_tensors='pt')\n",
    "        \n",
    "        post_input_ids       = tokenized_conts['input_ids'].squeeze()\n",
    "        post_attention_masks = tokenized_conts['attention_mask'].squeeze()\n",
    "\n",
    "        # print(type(self.lexicon_vectors))\n",
    "        # print(self.lexicon_vectors)\n",
    "        return { 'posts': {\n",
    "                          'text': self.posts[index],\n",
    "                          'tokenized_text': self.tokenized_posts[index],\n",
    "                          'lexicon_vector': torch.Tensor(self.lexicon_vectors[index]),\n",
    "                          'gold_attn_vectors': torch.Tensor(self.gold_attn_vectors[index]),\n",
    "                          'emoji_vector': torch.Tensor(self.emoji_vectors[index]),\n",
    "                          'input_ids': post_input_ids,\n",
    "                          'attention_masks': post_attention_masks\n",
    "                        },\n",
    "                \n",
    "                 'labels': torch.Tensor(self.labels[index])#.float()\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbcQzOpWd3-Q"
   },
   "source": [
    "#### Model Class - BERT + LSTM + Concatenation of Hidden Layers\n",
    "\n",
    "Parameters:\n",
    "\n",
    "\n",
    "*   Droput = 0.25\n",
    "*   LSTM Hidden Size = 64, bidirectional true so = 128\n",
    "*   Second last layer size = 128\n",
    "*   Last layer = 1 node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jD1H5h-BM0Jn"
   },
   "outputs": [],
   "source": [
    "class HO_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HO_Model, self).__init__()\n",
    "    \n",
    "        self.bert_model          = AutoModel.from_pretrained(config.MODEL_PATH, output_hidden_states = config.OUTPUT_HIDDEN_STATE, output_attentions = config.OUTPUT_ATTENTIONS)\n",
    "        self.dropout             = nn.Dropout(0.25)\n",
    "        \n",
    "        self.complete_mha = nn.MultiheadAttention(embed_dim = 50, num_heads=5)\n",
    "        \n",
    "        self.lstm_lexicon                  = nn.LSTM(input_size = 4, hidden_size = 32, batch_first=True, bidirectional=True )\n",
    "        self.linear_lexicon                = nn.Linear(64, 32)\n",
    "        self.linear_emoji                  = nn.Linear(300, 32)\n",
    "        \n",
    "        self.concat_fc1                    = nn.Linear(420, 128)\n",
    "        self.concat_fc2                    = nn.Linear(128, config.NUM_LABELS)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_masks, gold_attn_vectors=None, lexicon_vector=None, emoji_vector=None, hashtag_vecto=None):\n",
    "        \n",
    "        output_embeddings, hidden_states, attention = self.bert_model(input_ids = input_ids, attention_mask = attention_masks).values()\n",
    "\n",
    "        bert_attn = attention[-1]\n",
    "        \n",
    "        h_bert_attn = bert_attn[:,2,:50,:50]\n",
    "        o_bert_attn = bert_attn[:,3,:50,:50]\n",
    "\n",
    "        h_bert_attn = torch.mean(h_bert_attn, 1)\n",
    "        o_bert_attn = torch.mean(o_bert_attn, 1)\n",
    "\n",
    "        \n",
    "        concat_attn       = torch.cat((h_bert_attn, o_bert_attn),0)\n",
    "        concat_attn       = concat_attn.reshape(2, len(input_ids), 50)\n",
    "        concat_attn       = self.complete_mha(concat_attn, concat_attn, concat_attn)\n",
    "        concat_attn       = concat_attn[0].permute(1,0,2)\n",
    "        concat_attn       = torch.flatten(concat_attn, start_dim=1)                ### [16,100]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Main Stem --> Augmenting Lexicon Vector + Emoji Vector\n",
    "        \n",
    "        output_embeddings = torch.mean(output_embeddings, 1)                ### [16,256]\n",
    "\n",
    "        \n",
    "        lexicon_all_hidden_states, (lexicon_last_hidden_state, lexicon_cell_state) = self.lstm_lexicon(lexicon_vector) # [16, 50, 64]\n",
    "        lexicon_sum_hidden_states = torch.sum(lexicon_all_hidden_states, 1)  ### [16,64]\n",
    "        lexicon_drop = self.dropout(lexicon_sum_hidden_states)               ### [16,64]\n",
    "        lexicon_linear = self.linear_lexicon(lexicon_drop)                   ### [16,32]\n",
    "\n",
    "        emoji_vector      = self.linear_emoji(emoji_vector)                  ### [16,32]  (300 --> 32)\n",
    "        lex_emo           = torch.cat((lexicon_linear, emoji_vector),dim=1)  ### [16,64]\n",
    "        lex_emo_drop      = self.dropout(lex_emo)        \n",
    "\n",
    "        concat            = torch.cat((output_embeddings, lex_emo_drop,concat_attn),dim=1) ### [main_stem, d_out,  f_out,  h_out,  o_out]\n",
    "\n",
    "        model_outputs     = self.concat_fc1(concat)\n",
    "        model_outputs     = self.concat_fc2(model_outputs)\n",
    "        \n",
    "        \n",
    "        ### Supervised Attention Loss\n",
    "        model_attentions    = [[gold_attn_vectors[:,2,:], h_bert_attn],\n",
    "                               [gold_attn_vectors[:,3,:], o_bert_attn]]\n",
    "\n",
    "\n",
    "        return model_outputs, model_attentions\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DF_Model, self).__init__()\n",
    "    \n",
    "        self.bert_model          = AutoModel.from_pretrained(config.MODEL_PATH, output_hidden_states = config.OUTPUT_HIDDEN_STATE, output_attentions = config.OUTPUT_ATTENTIONS)\n",
    "        self.dropout             = nn.Dropout(0.25)\n",
    "        \n",
    "        self.complete_mha = nn.MultiheadAttention(embed_dim = 50, num_heads=5)\n",
    "        \n",
    "        self.lstm_lexicon                  = nn.LSTM(input_size = 4, hidden_size = 32, batch_first=True, bidirectional=True )\n",
    "        self.linear_lexicon                = nn.Linear(64, 32)\n",
    "        self.linear_emoji                  = nn.Linear(300, 32)\n",
    "        \n",
    "        self.concat_fc1                    = nn.Linear(420, 128)\n",
    "        self.concat_fc2                    = nn.Linear(128, config.NUM_LABELS)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_masks, gold_attn_vectors, lexicon_vector=None, emoji_vector=None, hashtag_vecto=None):\n",
    "        \n",
    "        output_embeddings, hidden_states, attention = self.bert_model(input_ids = input_ids, attention_mask = attention_masks).values()\n",
    "\n",
    "        bert_attn = attention[-1]\n",
    "        \n",
    "        d_bert_attn = bert_attn[:,0,:50,:50]\n",
    "        f_bert_attn = bert_attn[:,1,:50,:50]\n",
    "     \n",
    "        d_bert_attn = torch.mean(d_bert_attn, 1)\n",
    "        f_bert_attn = torch.mean(f_bert_attn, 1)\n",
    "        \n",
    "        concat_attn       = torch.cat((d_bert_attn, f_bert_attn),0)\n",
    "        concat_attn       = concat_attn.reshape(2, len(input_ids), 50)\n",
    "        concat_attn       = self.complete_mha(concat_attn, concat_attn, concat_attn)\n",
    "        concat_attn       = concat_attn[0].permute(1,0,2)\n",
    "        concat_attn       = torch.flatten(concat_attn, start_dim=1)                ### [16,100]\n",
    "        \n",
    "        \n",
    "        ### Main Stem --> Augmenting Lexicon Vector + Emoji Vector\n",
    "        \n",
    "        output_embeddings = torch.mean(output_embeddings, 1)                 ### [16,256]\n",
    "\n",
    "        \n",
    "        lexicon_all_hidden_states, (lexicon_last_hidden_state, lexicon_cell_state) = self.lstm_lexicon(lexicon_vector) # [16, 50, 64]\n",
    "        lexicon_sum_hidden_states = torch.sum(lexicon_all_hidden_states, 1)  ### [16,64]\n",
    "        lexicon_drop = self.dropout(lexicon_sum_hidden_states)               ### [16,64]\n",
    "        lexicon_linear = self.linear_lexicon(lexicon_drop)                   ### [16,32]\n",
    "\n",
    "        emoji_vector      = self.linear_emoji(emoji_vector)                  ### [16,32]  (300 --> 32)\n",
    "        lex_emo           = torch.cat((lexicon_linear, emoji_vector),dim=1)  ### [16,64]\n",
    "        lex_emo_drop      = self.dropout(lex_emo)        \n",
    "\n",
    "        concat            = torch.cat((output_embeddings, lex_emo_drop,concat_attn),dim=1) ### [main_stem, d_out,  f_out,  h_out,  o_out]\n",
    "\n",
    "        model_outputs     = self.concat_fc1(concat)\n",
    "        model_outputs     = self.concat_fc2(model_outputs)\n",
    "        \n",
    "        \n",
    "        ### Supervised Attention Loss\n",
    "        model_attentions    = [[gold_attn_vectors[:,0,:], d_bert_attn],\n",
    "                               [gold_attn_vectors[:,1,:], f_bert_attn]]\n",
    "\n",
    "\n",
    "        return model_outputs, model_attentions\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRzlhA2S2O7X"
   },
   "source": [
    "#### Predict Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(dataloader, checkpoint, labels, model_name=None ):\n",
    "    \n",
    "    print(\"Predicting Labels for : \\n\",model_name,\"\\n\\n\")\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "    if(model_name == \"Hate + Offensive\"):\n",
    "        model = HO_Model()\n",
    "        model.to(config.DEVICE)\n",
    "\n",
    "    if(model_name == \"Defamation + Fake\"):\n",
    "        model = DF_Model()\n",
    "        model.to(config.DEVICE)\n",
    "\n",
    "\n",
    "#     print(\"\\n\\n-----------------Model configuration----------------\\n\\n\",model,\"\\n\\n\\n\")\n",
    "\n",
    "    # define the parameters to be optmized and add regularization\n",
    "    if config.FULL_FINETUNING:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [    {\n",
    "                                        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                                        \"weight_decay\": 0.001,\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                                        \"weight_decay\": 0.0,\n",
    "                                    }]\n",
    "        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n",
    "\n",
    "    num_training_steps = len(train_dataloader) * config.EPOCHS\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=num_training_steps)\n",
    "    \n",
    "\n",
    "    if(checkpoint != None):\n",
    "        print(\"\\n\\n------------------- Loading Checkpoint-----------------------\\n\\n\")\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        max_val_f1_score = checkpoint['val_f1_score']\n",
    "        warnings.filterwarnings(\"default\")\n",
    "        print(\"\\n\\n------------------- Checkpoint Loaded-----------------------\\n\\n\")\n",
    "\n",
    "\n",
    "    pred  = []\n",
    "    \n",
    "    # set model.eval() every time during evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # unpack the batch contents and push them to the device (cuda or cpu).\n",
    "        b_input_ids       = batch['posts']['input_ids'].to(config.DEVICE)\n",
    "        b_attention_masks = batch['posts']['attention_masks'].to(config.DEVICE)\n",
    "        b_labels          = batch['labels'].to(config.DEVICE)\n",
    "\n",
    "        if(config.HOSTILITY_LEXICON):\n",
    "            b_lexicon     = batch['posts']['lexicon_vector'].to(config.DEVICE)\n",
    "        else:\n",
    "            b_lexicon     = None\n",
    "                 \n",
    "        if(config.EMOJI_INFO):\n",
    "            b_emoji       = batch['posts']['emoji_vector'].to(config.DEVICE)\n",
    "        else:\n",
    "            b_emoji       = None\n",
    "\n",
    "        if(config.HASHTAG_INFO):\n",
    "            b_hashtag     = None\n",
    "        else:\n",
    "            b_hashtag     = None\n",
    "            \n",
    "        if(config.SUPERVISED_LEXICON_ATTENTION):\n",
    "            b_gold_attn_vectors = batch['posts']['gold_attn_vectors'].to(config.DEVICE)\n",
    "        else:\n",
    "            b_gold_attn_vectors = None\n",
    "            \n",
    "            \n",
    "        # using torch.no_grad() during validation/inference is faster -\n",
    "        # - since it does not update gradients.\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # forward pass\n",
    "            logits, model_attentions = model(b_input_ids, b_attention_masks, b_gold_attn_vectors, b_lexicon,b_emoji,b_hashtag)\n",
    "\n",
    "            # since we're using BCEWithLogitsLoss, to get the predictions -\n",
    "            # - sigmoid has to be applied on the logits first\n",
    "            logits = torch.sigmoid(logits)\n",
    "            logits = np.round(logits.cpu().numpy())\n",
    "        \n",
    "            # the tensors are detached from the gpu and put back on -\n",
    "            # - the cpu, and then converted to numpy in order to -\n",
    "            # - use sklearn's metrics.\n",
    "\n",
    "            for item in logits:\n",
    "                pred.append(item[:2])\n",
    "                \n",
    "    print(len(pred))\n",
    "\n",
    "    return pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking  5728  only\n",
      "Taking  811  only\n",
      "Taking  1653  only\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(config.SEED)\n",
    "\n",
    "config.COMBINE_HATE_OFFENSIVE  = True\n",
    "config.COMBINE_DEFAMATION_FAKE = False\n",
    "config.NUM_LABELS = 3\n",
    "\n",
    "train_dataset = HindiHostilityDataset(\"Data/Old Pickles/pkl_fine_train.pkl\")\n",
    "val_dataset   = HindiHostilityDataset(\"Data/Old Pickles/pkl_fine_valid.pkl\")\n",
    "test_dataset  = HindiHostilityDataset(\"Data/Old Pickles/pkl_fine_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config.SEED)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3])\n",
      "torch.Size([16, 4, 50])\n",
      "torch.Size([16, 50, 4])\n",
      "torch.Size([16, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "\n",
    "b_texts             = batch['posts']['text']\n",
    "b_tokenized_texts   = batch['posts']['tokenized_text']\n",
    "b_input_ids         = batch['posts']['input_ids']\n",
    "b_attention_masks   = batch['posts']['attention_masks']\n",
    "b_lexicon           = batch['posts']['lexicon_vector']\n",
    "b_gold_attn_vectors = batch['posts']['gold_attn_vectors']\n",
    "b_emoji             = batch['posts']['emoji_vector']\n",
    "b_labels            = batch['labels']\n",
    "\n",
    "\n",
    "print(b_labels.shape)\n",
    "print(b_gold_attn_vectors.shape)\n",
    "print(b_lexicon.shape)\n",
    "print(b_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3379,  0.5068,  0.3279],\n",
       "        [-0.9140,  0.7598,  0.2832],\n",
       "        [-0.1779,  0.5209,  0.3080],\n",
       "        [-0.3962,  0.5482,  0.1996],\n",
       "        [-0.4645,  0.3286,  0.3209],\n",
       "        [-0.5997,  0.6249,  0.1806],\n",
       "        [-0.4810,  0.4724,  0.3608],\n",
       "        [-0.2981,  0.4703,  0.2339],\n",
       "        [-0.4881,  0.3831,  0.1130],\n",
       "        [-0.5429,  0.7935,  0.7432],\n",
       "        [-0.5459,  0.3077,  0.2132],\n",
       "        [-0.7081,  0.3350,  0.2339],\n",
       "        [-0.5634,  0.3643,  0.0875],\n",
       "        [-0.6795,  0.6869,  0.2196],\n",
       "        [-0.2786,  0.2684,  0.1079],\n",
       "        [-0.1590,  0.3773,  0.6171]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DF_Model()\n",
    "outputs, attentions = model(b_input_ids, b_attention_masks, b_gold_attn_vectors, b_lexicon,b_emoji)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITYdREJY56Cl"
   },
   "source": [
    "<h4> HO - Fine Grained Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9s5r0qYF00JX",
    "outputId": "85ba19ae-edbe-4d8b-87a9-fa746eb8c04c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking  5728  only\n",
      "Taking  811  only\n",
      "Taking  1653  only\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(config.SEED)\n",
    "\n",
    "config.COMBINE_HATE_OFFENSIVE  = True\n",
    "config.COMBINE_DEFAMATION_FAKE = False\n",
    "config.NUM_LABELS = 3\n",
    "\n",
    "train_dataset = HindiHostilityDataset(\"Data/Old Pickles/pkl_fine_train.pkl\")\n",
    "val_dataset   = HindiHostilityDataset(\"Data/Old Pickles/pkl_fine_valid.pkl\")\n",
    "test_dataset  = HindiHostilityDataset(\"Data/Old Pickles/pkl_fine_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GaeWpS3T00JX"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(config.SEED)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "FY1a7CI400JY"
   },
   "outputs": [],
   "source": [
    "config.FINE_TUNE_COARSE = False\n",
    "config.LOAD_CHECKPOINT  = True\n",
    "config.HO_CHECKPOINT_PATH  = \"Models/Final/Supervised Attention/Two Models/128 HO HindiBert 4BertAttn CustomLoss -  F1 - 0.6108531947502773.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6sfdcbye4iV",
    "outputId": "25b83935-313f-4589-f80e-7b02c8ea2e60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------HO Checkpoint Loaded-----------------\n",
      "\n",
      " \n",
      "Predicting Labels for : \n",
      " Hate + Offensive \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------- Loading Checkpoint-----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------- Checkpoint Loaded-----------------------\n",
      "\n",
      "\n",
      "1653\n"
     ]
    }
   ],
   "source": [
    "if(config.LOAD_CHECKPOINT):\n",
    "\n",
    "    ho_checkpoint_path = config.HO_CHECKPOINT_PATH\n",
    "    ho_checkpoint      = torch.load(ho_checkpoint_path, map_location=torch.device(config.DEVICE))\n",
    "    print(\"\\n\\n------------HO Checkpoint Loaded-----------------\\n\\n\", config.CHECKPOINT_PATH)\n",
    "\n",
    "    \n",
    "labels = [\"Hate\", \"Offensive\"]\n",
    "pred_HO = predict_labels(test_dataloader, ho_checkpoint, labels, model_name = \"Hate + Offensive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit19014/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1., 0.], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_HO[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> DF - Fine Grained Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9s5r0qYF00JX",
    "outputId": "85ba19ae-edbe-4d8b-87a9-fa746eb8c04c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-60d888139115>:5: ResourceWarning: unclosed file <_io.BufferedReader name='Data/Old Pickles/pkl_fine_train.pkl'>\n",
      "  df          = pickle.load(open(dataset_path, 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking  5728  only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-60d888139115>:5: ResourceWarning: unclosed file <_io.BufferedReader name='Data/Old Pickles/pkl_fine_valid.pkl'>\n",
      "  df          = pickle.load(open(dataset_path, 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking  811  only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-60d888139115>:5: ResourceWarning: unclosed file <_io.BufferedReader name='Data/Old Pickles/pkl_fine_test.pkl'>\n",
      "  df          = pickle.load(open(dataset_path, 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking  1653  only\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(config.SEED)\n",
    "\n",
    "config.COMBINE_DEFAMATION_FAKE  = True\n",
    "config.COMBINE_HATE_OFFENSIVE   = False\n",
    "config.NUM_LABELS = 3\n",
    "\n",
    "train_dataset = HindiHostilityDataset(\"Data/Old Pickles/pkl_fine_train.pkl\")\n",
    "val_dataset   = HindiHostilityDataset(\"Data/Old Pickles/pkl_fine_valid.pkl\")\n",
    "test_dataset  = HindiHostilityDataset(\"Data/Old Pickles/pkl_fine_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "GaeWpS3T00JX"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(config.SEED)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.LOAD_CHECKPOINT  = True\n",
    "config.DF_CHECKPOINT_PATH  = \"Models/Final/Supervised Attention/Two Models/128 DF HindiBert 4BertAttn CustomLoss -  F1 - 0.6127511129760648.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------DF Checkpoint Loaded-----------------\n",
      "\n",
      " \n",
      "Predicting Labels for : \n",
      " Defamation + Fake \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------- Loading Checkpoint-----------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------------------- Checkpoint Loaded-----------------------\n",
      "\n",
      "\n",
      "1653\n"
     ]
    }
   ],
   "source": [
    "if(config.LOAD_CHECKPOINT):\n",
    "    df_checkpoint_path = config.DF_CHECKPOINT_PATH\n",
    "    df_checkpoint      = torch.load(df_checkpoint_path, map_location=torch.device(config.DEVICE))    \n",
    "    print(\"\\n\\n------------DF Checkpoint Loaded-----------------\\n\\n\", config.CHECKPOINT_PATH)\n",
    "\n",
    "\n",
    "labels = [\"Defamation\", \"Fake\"]\n",
    "pred_DF = predict_labels(test_dataloader, df_checkpoint, labels, model_name = \"Defamation + Fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit19014/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0., 0.], dtype=float32),\n",
       " array([0., 1.], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([0., 1.], dtype=float32),\n",
       " array([0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_DF[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0.], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_HO[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for a,b in zip(pred_DF, pred_HO):\n",
    "    final_label = []\n",
    "    for item in a:\n",
    "        final_label.append(item)\n",
    "    for item in b:\n",
    "        final_label.append(item)\n",
    "    \n",
    "    pred.append(final_label)\n",
    "    \n",
    "print(pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-7d29c5f1e6b5>:1: ResourceWarning: unclosed file <_io.BufferedReader name='Data/pkl_fine_test.pkl'>\n",
      "  test_df = pickle.load(open(\"Data/pkl_fine_test.pkl\", 'rb'))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 1.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Post</th>\n",
       "      <th>Processed With Emoji</th>\n",
       "      <th>Processed Without Emoji</th>\n",
       "      <th>Tokenized Post</th>\n",
       "      <th>All IndicFT Embeddings</th>\n",
       "      <th>Padded IndicFT Embeddings</th>\n",
       "      <th>Softmaxed Lex Padded</th>\n",
       "      <th>Softmaxed Lex Average</th>\n",
       "      <th>Softmaxed Lex Mask</th>\n",
       "      <th>...</th>\n",
       "      <th>emojis_list</th>\n",
       "      <th>replace_emoji</th>\n",
       "      <th>emoji2text</th>\n",
       "      <th>emoji2vec</th>\n",
       "      <th>Hostile</th>\n",
       "      <th>Defamation</th>\n",
       "      <th>Fake</th>\n",
       "      <th>Hate</th>\n",
       "      <th>Offensive</th>\n",
       "      <th>Labels Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>         ...</td>\n",
       "      <td>         ...</td>\n",
       "      <td>         ...</td>\n",
       "      <td>         ...</td>\n",
       "      <td>[[-0.010175878, 0.17647043, 0.32597065, -0.152...</td>\n",
       "      <td>[[-0.010175878, 0.17647043, 0.32597065, -0.152...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[0.12281525615536089, 0.10788757219638383, 0.1...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>         ...</td>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>defamation,fake,offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>: BMP       ...</td>\n",
       "      <td> BMP        ...</td>\n",
       "      <td> BMP        ...</td>\n",
       "      <td>       ...</td>\n",
       "      <td>[[-0.02822564, -0.34272358, 0.015664268, -0.26...</td>\n",
       "      <td>[[-0.02822564, -0.34272358, 0.015664268, -0.26...</td>\n",
       "      <td>[[0.21712413297477984, 0.4650181247330267, 0.1...</td>\n",
       "      <td>[0.10860063058899874, 0.2558063704043224, 0.10...</td>\n",
       "      <td>[1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, ...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>: BMP       ...</td>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>non-hostile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>  ,   ,   , ...</td>\n",
       "      <td>         ...</td>\n",
       "      <td>         ...</td>\n",
       "      <td>        ...</td>\n",
       "      <td>[[0.076749325, 0.09945227, 0.01090258, 0.20269...</td>\n",
       "      <td>[[0.076749325, 0.09945227, 0.01090258, 0.20269...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [...</td>\n",
       "      <td>[0.1490461053213771, 0.19102224971784912, 0.10...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, ...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>  ,   ,   , ...</td>\n",
       "      <td>folded_hands face_with_tears_of_joy thumbs_up</td>\n",
       "      <td>[0.045116836205124855, -6.391700298991054e-05,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>        ...</td>\n",
       "      <td>        ...</td>\n",
       "      <td>        ...</td>\n",
       "      <td>         ...</td>\n",
       "      <td>[[0.24119985, -0.019852951, -0.09618474, 0.396...</td>\n",
       "      <td>[[0.24119985, -0.019852951, -0.09618474, 0.396...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0], [0.25, 0.25, 0.25, 0.25...</td>\n",
       "      <td>[0.2565532888839337, 0.16772352341047086, 0.09...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, ...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>        ...</td>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>RT @_Pb_swain_:        ...</td>\n",
       "      <td>RT someuser        ...</td>\n",
       "      <td>RT someuser        ...</td>\n",
       "      <td>           ...</td>\n",
       "      <td>[[0.015281931, 0.2259393, -0.053103715, 0.0909...</td>\n",
       "      <td>[[0.015281931, 0.2259393, -0.053103715, 0.0909...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0], [0.20008834622120195, 0...</td>\n",
       "      <td>[0.13704723817167858, 0.13018843364746632, 0.1...</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, ...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>RT @_Pb_swain_:        ...</td>\n",
       "      <td>backhand_index_pointing_down face_with_tears_o...</td>\n",
       "      <td>[-0.00757233202457428, -0.017534792050719263, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>defamation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique ID                                               Post  \\\n",
       "0          1           ...   \n",
       "1          2  : BMP       ...   \n",
       "2          3    ,   ,   , ...   \n",
       "3          4          ...   \n",
       "4          5  RT @_Pb_swain_:        ...   \n",
       "\n",
       "                                Processed With Emoji  \\\n",
       "0           ...   \n",
       "1   BMP        ...   \n",
       "2           ...   \n",
       "3          ...   \n",
       "4  RT someuser        ...   \n",
       "\n",
       "                             Processed Without Emoji  \\\n",
       "0           ...   \n",
       "1   BMP        ...   \n",
       "2           ...   \n",
       "3          ...   \n",
       "4  RT someuser        ...   \n",
       "\n",
       "                                      Tokenized Post  \\\n",
       "0           ...   \n",
       "1         ...   \n",
       "2          ...   \n",
       "3           ...   \n",
       "4             ...   \n",
       "\n",
       "                              All IndicFT Embeddings  \\\n",
       "0  [[-0.010175878, 0.17647043, 0.32597065, -0.152...   \n",
       "1  [[-0.02822564, -0.34272358, 0.015664268, -0.26...   \n",
       "2  [[0.076749325, 0.09945227, 0.01090258, 0.20269...   \n",
       "3  [[0.24119985, -0.019852951, -0.09618474, 0.396...   \n",
       "4  [[0.015281931, 0.2259393, -0.053103715, 0.0909...   \n",
       "\n",
       "                           Padded IndicFT Embeddings  \\\n",
       "0  [[-0.010175878, 0.17647043, 0.32597065, -0.152...   \n",
       "1  [[-0.02822564, -0.34272358, 0.015664268, -0.26...   \n",
       "2  [[0.076749325, 0.09945227, 0.01090258, 0.20269...   \n",
       "3  [[0.24119985, -0.019852951, -0.09618474, 0.396...   \n",
       "4  [[0.015281931, 0.2259393, -0.053103715, 0.0909...   \n",
       "\n",
       "                                Softmaxed Lex Padded  \\\n",
       "0  [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [...   \n",
       "1  [[0.21712413297477984, 0.4650181247330267, 0.1...   \n",
       "2  [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [...   \n",
       "3  [[0.0, 0.0, 0.0, 0.0], [0.25, 0.25, 0.25, 0.25...   \n",
       "4  [[0.0, 0.0, 0.0, 0.0], [0.20008834622120195, 0...   \n",
       "\n",
       "                               Softmaxed Lex Average  \\\n",
       "0  [0.12281525615536089, 0.10788757219638383, 0.1...   \n",
       "1  [0.10860063058899874, 0.2558063704043224, 0.10...   \n",
       "2  [0.1490461053213771, 0.19102224971784912, 0.10...   \n",
       "3  [0.2565532888839337, 0.16772352341047086, 0.09...   \n",
       "4  [0.13704723817167858, 0.13018843364746632, 0.1...   \n",
       "\n",
       "                                  Softmaxed Lex Mask  ... emojis_list  \\\n",
       "0  [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, ...  ...               \n",
       "1  [1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, ...  ...               \n",
       "2  [0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, ...  ...            \n",
       "3  [0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, ...  ...               \n",
       "4  [0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, ...  ...          \n",
       "\n",
       "                                       replace_emoji  \\\n",
       "0           ...   \n",
       "1  : BMP       ...   \n",
       "2    ,   ,   , ...   \n",
       "3          ...   \n",
       "4  RT @_Pb_swain_:        ...   \n",
       "\n",
       "                                          emoji2text  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2     folded_hands face_with_tears_of_joy thumbs_up    \n",
       "3                                                      \n",
       "4  backhand_index_pointing_down face_with_tears_o...   \n",
       "\n",
       "                                           emoji2vec Hostile Defamation Fake  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       1          1    1   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       0          0    0   \n",
       "2  [0.045116836205124855, -6.391700298991054e-05,...       1          0    0   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...       1          0    1   \n",
       "4  [-0.00757233202457428, -0.017534792050719263, ...       1          1    0   \n",
       "\n",
       "  Hate Offensive                 Labels Set  \n",
       "0    0         1  defamation,fake,offensive  \n",
       "1    0         0                non-hostile  \n",
       "2    1         0                       hate  \n",
       "3    0         0                       fake  \n",
       "4    0         0                 defamation  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pickle.load(open(\"Data/pkl_fine_test.pkl\", 'rb'))\n",
    "# test_df = test_df[test_df['Hostile']==1]\n",
    "# test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "config.COMBINE_DEFAMATION_FAKE  = False\n",
    "config.COMBINE_HATE_OFFENSIVE   = False\n",
    "config.NUM_LABELS = 4\n",
    "\n",
    "true = get_labels(test_df)\n",
    "print(true[:5])\n",
    "test_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weighted F1 - Score (For Fine Grained - Hostile Posts Only):  [0.26724138 0.42050391 0.41608392 0.46021505]\n",
      "\n",
      " Weighted F1 - Score (For Fine Grained - Hostile Posts Only):  0.40142555655052653\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.18      0.27       169\n",
      "           1       0.30      0.72      0.42       334\n",
      "           2       0.35      0.51      0.42       234\n",
      "           3       0.43      0.49      0.46       219\n",
      "\n",
      "   micro avg       0.34      0.52      0.41       956\n",
      "   macro avg       0.39      0.48      0.39       956\n",
      "weighted avg       0.38      0.52      0.40       956\n",
      " samples avg       0.26      0.26      0.25       956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit19014/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(true, pred, average=None)\n",
    "print(\"\\n Weighted F1 - Score (For Fine Grained - Hostile Posts Only): \", f1)\n",
    "f1 = f1_score(true, pred, average='weighted')\n",
    "print(\"\\n Weighted F1 - Score (For Fine Grained - Hostile Posts Only): \", f1)\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"\\n\\n\",classification_report(true,pred))\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1653, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Labels Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>fake,hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique ID Labels Set\n",
       "0          1       fake\n",
       "1          2       fake\n",
       "2          3  fake,hate\n",
       "3          4       fake\n",
       "4          5       fake"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "\n",
    "for id,labels in enumerate(pred):\n",
    "    label_vector = [0,\"\"]\n",
    "    \n",
    "    label_vector[0] = id+1\n",
    "    \n",
    "    for i,label in enumerate(labels):\n",
    "        \n",
    "        if(i==0 and label==1.0):\n",
    "            label_vector[1] += \"defamation,\"\n",
    "            \n",
    "        if(i==1 and label==1.0):\n",
    "            label_vector[1] += \"fake,\"\n",
    "            \n",
    "        if(i==2 and label==1.0):\n",
    "            label_vector[1] += \"hate,\"\n",
    "            \n",
    "        if(i==3 and label==1.0):\n",
    "            label_vector[1] += \"offensive,\"\n",
    "            \n",
    "    if(label_vector[1] == \"\"):\n",
    "        label_vector[1] = \"non-hostile \"\n",
    "    \n",
    "\n",
    "    \n",
    "    label_vector[1] = label_vector[1][:-1]\n",
    "    rows.append(label_vector)\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(data = rows, columns= [\"Unique ID\", \"Labels Set\"])\n",
    "print(df.shape)\n",
    "df.head()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mCode\u001b[0m/  \u001b[01;34mData\u001b[0m/  \u001b[01;34mDumps\u001b[0m/  \u001b[01;34mModels\u001b[0m/  \u001b[01;34mPhotos\u001b[0m/  \u001b[01;34mTensorboard\u001b[0m/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit19014/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Dumps/ensemble_check.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
